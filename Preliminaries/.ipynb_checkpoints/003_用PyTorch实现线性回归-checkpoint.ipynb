{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤\n",
    "1、prepare dataset\n",
    "\n",
    "2、design model using Class  # 目的是为了前向传播forward，即计算y hat(预测值)\n",
    "\n",
    "3、Construct loss and optimizer (using PyTorch API) 其中，计算loss是为了进行反向传播，optimizer是为了更新梯度。\n",
    "\n",
    "4、Training cycle (forward,backward,update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = xw + b   行表示样本个数，列代表特征数量  如 y是n * 3 的矩阵，x是n*2的矩阵，那么w是2*3的矩阵\n",
    "\n",
    "如果写成转置 y = w^T * x + b 行代表特征数量，列代表样本个数，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码说明\n",
    "1、Module（可调用类）实现了魔法函数__call__()，call()里面有一条语句是要调用forward()。因此新写的类中需要重写forward()覆盖掉父类中的forward()\n",
    "\n",
    "2、call函数的另一个作用是可以直接在对象后面加()，例如实例化的model对象，和实例化的linear对象\n",
    "\n",
    "3、本算法的forward体现是通过以下语句实现的：\n",
    "\n",
    "```y_pred = model(x_data)```\n",
    "\n",
    "由于魔法函数call的实现,model(x_data)将会调用model.forward(x_data)函数，\n",
    "\n",
    "model.forward(x_data)函数中的y_pred = self.linear(x)\n",
    "\n",
    "self.linear(x)也由于魔法函数call的实现将会调用torch.nn.Linear类中的forward，至此完成封装，也就是说forward最终是在torch.nn.Linear类中实现的，具体怎么实现，可以不用关心，大概就是y= wx + b。\n",
    "\n",
    "4、本算法的反向传播，计算梯度是通过以下语句实现的：\n",
    "\n",
    "```loss.backward() # 反向传播，计算梯度```\n",
    "\n",
    "5、本算法的参数(w,b)更新，是通过以下语句实现的：\n",
    "\n",
    "```optimizer.step() # update 参数，即更新w和b的值```\n",
    "\n",
    "6、 每一次epoch的训练过程，总结就是\n",
    "\n",
    "①前向传播，求y hat （输入的预测值）\n",
    "\n",
    "②根据y_hat和y_label(y_data)计算loss\n",
    "\n",
    "③反向传播 backward (计算梯度)\n",
    "\n",
    "④根据梯度，更新参数\n",
    "\n",
    "7、本实例是批量数据处理，小伙伴们不要被optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)误导了，以为见了SGD就是随机梯度下降。要看传进来的数据是单个的还是批量的。这里的x_data是3个数据，是一个batch，调用的PyTorch API是 torch.optim.SGD，但这里的SGD不是随机梯度下降，而是批量梯度下降。也就是说，梯度下降算法使用的是随机梯度下降，还是批量梯度下降，还是mini-batch梯度下降，用的API都是 torch.optim.SGD。\n",
    "\n",
    "8、torch.nn.MSELoss也跟torch.nn.Module有关，参与计算图的构建，torch.optim.SGD与torch.nn.Module无关，不参与构建计算图。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Linear in module torch.nn.modules.linear:\n",
      "\n",
      "class Linear(torch.nn.modules.module.Module)\n",
      " |  Linear(in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None\n",
      " |  \n",
      " |  Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
      " |  \n",
      " |  This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      " |  \n",
      " |  Args:\n",
      " |      in_features: size of each input sample\n",
      " |      out_features: size of each output sample\n",
      " |      bias: If set to ``False``, the layer will not learn an additive bias.\n",
      " |          Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
      " |        dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
      " |      - Output: :math:`(*, H_{out})` where all but the last dimension\n",
      " |        are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight: the learnable weights of the module of shape\n",
      " |          :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      " |          initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      " |          :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      " |      bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      " |              If :attr:`bias` is ``True``, the values are initialized from\n",
      " |              :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      " |              :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> m = nn.Linear(20, 30)\n",
      " |      >>> input = torch.randn(128, 20)\n",
      " |      >>> output = m(input)\n",
      " |      >>> print(output.size())\n",
      " |      torch.Size([128, 30])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Linear\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  forward(self, input: torch.Tensor) -> torch.Tensor\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'in_features': <class 'int'>, 'out_features': <clas...\n",
      " |  \n",
      " |  __constants__ = ['in_features', 'out_features']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be pickleable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MSELoss in module torch.nn.modules.loss:\n",
      "\n",
      "class MSELoss(_Loss)\n",
      " |  MSELoss(size_average=None, reduce=None, reduction: str = 'mean') -> None\n",
      " |  \n",
      " |  Creates a criterion that measures the mean squared error (squared L2 norm) between\n",
      " |  each element in the input :math:`x` and target :math:`y`.\n",
      " |  \n",
      " |  The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n",
      " |  \n",
      " |  .. math::\n",
      " |      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
      " |      l_n = \\left( x_n - y_n \\right)^2,\n",
      " |  \n",
      " |  where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``\n",
      " |  (default ``'mean'``), then:\n",
      " |  \n",
      " |  .. math::\n",
      " |      \\ell(x, y) =\n",
      " |      \\begin{cases}\n",
      " |          \\operatorname{mean}(L), &  \\text{if reduction} = \\text{`mean';}\\\\\n",
      " |          \\operatorname{sum}(L),  &  \\text{if reduction} = \\text{`sum'.}\n",
      " |      \\end{cases}\n",
      " |  \n",
      " |  :math:`x` and :math:`y` are tensors of arbitrary shapes with a total\n",
      " |  of :math:`n` elements each.\n",
      " |  \n",
      " |  The mean operation still operates over all the elements, and divides by :math:`n`.\n",
      " |  \n",
      " |  The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.\n",
      " |  \n",
      " |  Args:\n",
      " |      size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      " |          the losses are averaged over each loss element in the batch. Note that for\n",
      " |          some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
      " |          is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      " |          when :attr:`reduce` is ``False``. Default: ``True``\n",
      " |      reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      " |          losses are averaged or summed over observations for each minibatch depending\n",
      " |          on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      " |          batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      " |      reduction (string, optional): Specifies the reduction to apply to the output:\n",
      " |          ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      " |          ``'mean'``: the sum of the output will be divided by the number of\n",
      " |          elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      " |          and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      " |          specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(*)`, where :math:`*` means any number of dimensions.\n",
      " |      - Target: :math:`(*)`, same shape as the input.\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> loss = nn.MSELoss()\n",
      " |      >>> input = torch.randn(3, 5, requires_grad=True)\n",
      " |      >>> target = torch.randn(3, 5)\n",
      " |      >>> output = loss(input, target)\n",
      " |      >>> output.backward()\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MSELoss\n",
      " |      _Loss\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __constants__ = ['reduction']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _Loss:\n",
      " |  \n",
      " |  __annotations__ = {'reduction': <class 'str'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be pickleable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.nn.MSELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGD in module torch.optim.sgd:\n",
      "\n",
      "class SGD(torch.optim.optimizer.Optimizer)\n",
      " |  SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False)\n",
      " |  \n",
      " |  Implements stochastic gradient descent (optionally with momentum).\n",
      " |  \n",
      " |  .. math::\n",
      " |     \\begin{aligned}\n",
      " |          &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      " |          &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
      " |              \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n",
      " |          &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n",
      " |          \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
      " |          &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
      " |          &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
      " |          &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n",
      " |          &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
      " |          &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n",
      " |          &\\hspace{10mm}\\textbf{else}                                                          \\\\\n",
      " |          &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n",
      " |          &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n",
      " |          &\\hspace{15mm} g_t \\leftarrow g_{t-1} + \\mu \\textbf{b}_t                             \\\\\n",
      " |          &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n",
      " |          &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n",
      " |          &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}                                          \\\\\n",
      " |          &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t                   \\\\[-1.ex]\n",
      " |          &\\hspace{5mm}\\textbf{else}                                                    \\\\[-1.ex]\n",
      " |          &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                   \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      " |          &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
      " |          &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
      " |     \\end{aligned}\n",
      " |  \n",
      " |  Nesterov momentum is based on the formula from\n",
      " |  `On the importance of initialization and momentum in deep learning`__.\n",
      " |  \n",
      " |  Args:\n",
      " |      params (iterable): iterable of parameters to optimize or dicts defining\n",
      " |          parameter groups\n",
      " |      lr (float): learning rate\n",
      " |      momentum (float, optional): momentum factor (default: 0)\n",
      " |      weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
      " |      dampening (float, optional): dampening for momentum (default: 0)\n",
      " |      nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
      " |      maximize (bool, optional): maximize the params based on the objective, instead of\n",
      " |          minimizing (default: False)\n",
      " |  \n",
      " |  Example:\n",
      " |      >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
      " |      >>> optimizer.zero_grad()\n",
      " |      >>> loss_fn(model(input), target).backward()\n",
      " |      >>> optimizer.step()\n",
      " |  \n",
      " |  __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
      " |  \n",
      " |  .. note::\n",
      " |      The implementation of SGD with Momentum/Nesterov subtly differs from\n",
      " |      Sutskever et. al. and implementations in some other frameworks.\n",
      " |  \n",
      " |      Considering the specific case of Momentum, the update can be written as\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n",
      " |      parameters, gradient, velocity, and momentum respectively.\n",
      " |  \n",
      " |      This is in contrast to Sutskever et. al. and\n",
      " |      other frameworks which employ an update of the form\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
      " |              p_{t+1} & = p_{t} - v_{t+1}.\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |      The Nesterov version is analogously modified.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SGD\n",
      " |      torch.optim.optimizer.Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  step(self, closure=None)\n",
      " |      Performs a single optimization step.\n",
      " |      \n",
      " |      Args:\n",
      " |          closure (callable, optional): A closure that reevaluates the model\n",
      " |              and returns the loss.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_param_group(self, param_group)\n",
      " |      Add a param group to the :class:`Optimizer` s `param_groups`.\n",
      " |      \n",
      " |      This can be useful when fine tuning a pre-trained network as frozen layers can be made\n",
      " |      trainable and added to the :class:`Optimizer` as training progresses.\n",
      " |      \n",
      " |      Args:\n",
      " |          param_group (dict): Specifies what Tensors should be optimized along with group\n",
      " |              specific optimization options.\n",
      " |  \n",
      " |  load_state_dict(self, state_dict)\n",
      " |      Loads the optimizer state.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): optimizer state. Should be an object returned\n",
      " |              from a call to :meth:`state_dict`.\n",
      " |  \n",
      " |  state_dict(self)\n",
      " |      Returns the state of the optimizer as a :class:`dict`.\n",
      " |      \n",
      " |      It contains two entries:\n",
      " |      \n",
      " |      * state - a dict holding current optimization state. Its content\n",
      " |          differs between optimizer classes.\n",
      " |      * param_groups - a list containing all parameter groups where each\n",
      " |          parameter group is a dict\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False)\n",
      " |      Sets the gradients of all optimized :class:`torch.Tensor` s to zero.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              This will in general have lower memory footprint, and can modestly improve performance.\n",
      " |              However, it changes certain behaviors. For example:\n",
      " |              1. When the user tries to access a gradient and perform manual ops on it,\n",
      " |              a None attribute or a Tensor full of 0s will behave differently.\n",
      " |              2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\\ s\n",
      " |              are guaranteed to be None for params that did not receive a gradient.\n",
      " |              3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None\n",
      " |              (in one case it does the step with a gradient of 0 and in the other it skips\n",
      " |              the step altogether).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.optim.optimizer.Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11.067163467407227\n",
      "w =  1.0196045637130737\n",
      "b =  0.911483883857727\n",
      "1 5.225486755371094\n",
      "w =  1.184737205505371\n",
      "b =  0.9744423031806946\n",
      "2 2.6206421852111816\n",
      "w =  1.2960777282714844\n",
      "b =  1.0138072967529297\n",
      "3 1.4568092823028564\n",
      "w =  1.3715190887451172\n",
      "b =  1.0374494791030884\n",
      "4 0.9345337152481079\n",
      "w =  1.4229998588562012\n",
      "b =  1.050620198249817\n",
      "5 0.6979207396507263\n",
      "w =  1.45848548412323\n",
      "b =  1.0568230152130127\n",
      "6 0.5885359048843384\n",
      "w =  1.4832907915115356\n",
      "b =  1.0583953857421875\n",
      "7 0.535848081111908\n",
      "w =  1.5009618997573853\n",
      "b =  1.0568968057632446\n",
      "8 0.5084576606750488\n",
      "w =  1.5138649940490723\n",
      "b =  1.0533676147460938\n",
      "9 0.49238497018814087\n",
      "w =  1.5235786437988281\n",
      "b =  1.04850172996521\n",
      "10 0.48140642046928406\n",
      "w =  1.5311564207077026\n",
      "b =  1.0427621603012085\n",
      "11 0.47275084257125854\n",
      "w =  1.5373011827468872\n",
      "b =  1.0364576578140259\n",
      "12 0.4651833772659302\n",
      "w =  1.5424818992614746\n",
      "b =  1.0297940969467163\n",
      "13 0.4581538438796997\n",
      "w =  1.5470116138458252\n",
      "b =  1.0229085683822632\n",
      "14 0.4514157176017761\n",
      "w =  1.5510993003845215\n",
      "b =  1.0158926248550415\n",
      "15 0.44486021995544434\n",
      "w =  1.554884433746338\n",
      "b =  1.0088071823120117\n",
      "16 0.4384365975856781\n",
      "w =  1.5584598779678345\n",
      "b =  1.0016926527023315\n",
      "17 0.4321218729019165\n",
      "w =  1.5618879795074463\n",
      "b =  0.9945759177207947\n",
      "18 0.425905704498291\n",
      "w =  1.565210223197937\n",
      "b =  0.987474799156189\n",
      "19 0.4197821319103241\n",
      "w =  1.568454384803772\n",
      "b =  0.9804010987281799\n",
      "20 0.4137481153011322\n",
      "w =  1.571639060974121\n",
      "b =  0.9733625054359436\n",
      "21 0.4078012704849243\n",
      "w =  1.5747766494750977\n",
      "b =  0.9663640856742859\n",
      "22 0.4019404351711273\n",
      "w =  1.5778754949569702\n",
      "b =  0.9594090580940247\n",
      "23 0.3961637020111084\n",
      "w =  1.5809413194656372\n",
      "b =  0.9524994492530823\n",
      "24 0.3904702365398407\n",
      "w =  1.5839778184890747\n",
      "b =  0.945636510848999\n",
      "25 0.3848586678504944\n",
      "w =  1.5869876146316528\n",
      "b =  0.9388209581375122\n",
      "26 0.3793274164199829\n",
      "w =  1.5899726152420044\n",
      "b =  0.9320532083511353\n",
      "27 0.3738758862018585\n",
      "w =  1.5929338932037354\n",
      "b =  0.9253333210945129\n",
      "28 0.3685024380683899\n",
      "w =  1.595872402191162\n",
      "b =  0.9186612367630005\n",
      "29 0.3632066249847412\n",
      "w =  1.5987887382507324\n",
      "b =  0.9120368957519531\n",
      "30 0.3579868674278259\n",
      "w =  1.601683497428894\n",
      "b =  0.9054600596427917\n",
      "31 0.35284218192100525\n",
      "w =  1.604556918144226\n",
      "b =  0.8989304304122925\n",
      "32 0.3477711081504822\n",
      "w =  1.6074093580245972\n",
      "b =  0.8924477696418762\n",
      "33 0.3427731394767761\n",
      "w =  1.6102410554885864\n",
      "b =  0.8860117793083191\n",
      "34 0.33784690499305725\n",
      "w =  1.6130521297454834\n",
      "b =  0.8796221613883972\n",
      "35 0.3329917788505554\n",
      "w =  1.6158429384231567\n",
      "b =  0.8732785582542419\n",
      "36 0.3282057046890259\n",
      "w =  1.6186134815216064\n",
      "b =  0.8669806718826294\n",
      "37 0.32348906993865967\n",
      "w =  1.6213639974594116\n",
      "b =  0.8607282042503357\n",
      "38 0.3188399374485016\n",
      "w =  1.6240947246551514\n",
      "b =  0.854520857334137\n",
      "39 0.3142578601837158\n",
      "w =  1.6268056631088257\n",
      "b =  0.8483582735061646\n",
      "40 0.3097415864467621\n",
      "w =  1.6294970512390137\n",
      "b =  0.8422400951385498\n",
      "41 0.3052901327610016\n",
      "w =  1.6321691274642944\n",
      "b =  0.8361660242080688\n",
      "42 0.3009026050567627\n",
      "w =  1.634821891784668\n",
      "b =  0.8301357626914978\n",
      "43 0.2965780198574066\n",
      "w =  1.6374554634094238\n",
      "b =  0.8241490125656128\n",
      "44 0.2923157513141632\n",
      "w =  1.6400700807571411\n",
      "b =  0.8182054162025452\n",
      "45 0.2881146967411041\n",
      "w =  1.6426657438278198\n",
      "b =  0.812304675579071\n",
      "46 0.2839741110801697\n",
      "w =  1.6452428102493286\n",
      "b =  0.8064464926719666\n",
      "47 0.27989262342453003\n",
      "w =  1.6478012800216675\n",
      "b =  0.8006305694580078\n",
      "48 0.27587053179740906\n",
      "w =  1.650341272354126\n",
      "b =  0.7948565483093262\n",
      "49 0.27190548181533813\n",
      "w =  1.6528629064559937\n",
      "b =  0.7891241908073425\n",
      "50 0.26799798011779785\n",
      "w =  1.6553664207458496\n",
      "b =  0.783433198928833\n",
      "51 0.26414617896080017\n",
      "w =  1.6578518152236938\n",
      "b =  0.777783215045929\n",
      "52 0.2603500187397003\n",
      "w =  1.6603193283081055\n",
      "b =  0.7721740007400513\n",
      "53 0.25660842657089233\n",
      "w =  1.662769079208374\n",
      "b =  0.7666052579879761\n",
      "54 0.25292059779167175\n",
      "w =  1.6652010679244995\n",
      "b =  0.7610766291618347\n",
      "55 0.24928584694862366\n",
      "w =  1.667615532875061\n",
      "b =  0.7555879354476929\n",
      "56 0.24570301175117493\n",
      "w =  1.6700125932693481\n",
      "b =  0.7501388192176819\n",
      "57 0.24217207729816437\n",
      "w =  1.6723923683166504\n",
      "b =  0.7447289824485779\n",
      "58 0.23869165778160095\n",
      "w =  1.6747549772262573\n",
      "b =  0.739358127117157\n",
      "59 0.23526135087013245\n",
      "w =  1.677100658416748\n",
      "b =  0.7340260744094849\n",
      "60 0.23188012838363647\n",
      "w =  1.6794294118881226\n",
      "b =  0.7287324666976929\n",
      "61 0.22854769229888916\n",
      "w =  1.6817412376403809\n",
      "b =  0.7234770059585571\n",
      "62 0.2252630740404129\n",
      "w =  1.6840364933013916\n",
      "b =  0.7182594537734985\n",
      "63 0.22202575206756592\n",
      "w =  1.6863151788711548\n",
      "b =  0.7130795121192932\n",
      "64 0.21883493661880493\n",
      "w =  1.68857741355896\n",
      "b =  0.7079369425773621\n",
      "65 0.21569009125232697\n",
      "w =  1.6908233165740967\n",
      "b =  0.7028314471244812\n",
      "66 0.2125902771949768\n",
      "w =  1.6930530071258545\n",
      "b =  0.6977627873420715\n",
      "67 0.20953483879566193\n",
      "w =  1.695266604423523\n",
      "b =  0.6927306652069092\n",
      "68 0.2065235674381256\n",
      "w =  1.6974643468856812\n",
      "b =  0.687734842300415\n",
      "69 0.20355549454689026\n",
      "w =  1.6996461153030396\n",
      "b =  0.6827750205993652\n",
      "70 0.20062991976737976\n",
      "w =  1.7018121480941772\n",
      "b =  0.6778509616851807\n",
      "71 0.19774669408798218\n",
      "w =  1.7039626836776733\n",
      "b =  0.6729624271392822\n",
      "72 0.19490475952625275\n",
      "w =  1.7060977220535278\n",
      "b =  0.6681091785430908\n",
      "73 0.1921035349369049\n",
      "w =  1.7082172632217407\n",
      "b =  0.6632909178733826\n",
      "74 0.18934261798858643\n",
      "w =  1.7103215456008911\n",
      "b =  0.6585074067115784\n",
      "75 0.18662168085575104\n",
      "w =  1.712410569190979\n",
      "b =  0.6537583470344543\n",
      "76 0.18393948674201965\n",
      "w =  1.7144845724105835\n",
      "b =  0.6490435600280762\n",
      "77 0.18129616975784302\n",
      "w =  1.7165436744689941\n",
      "b =  0.6443628072738647\n",
      "78 0.1786906123161316\n",
      "w =  1.718587875366211\n",
      "b =  0.6397157907485962\n",
      "79 0.17612244188785553\n",
      "w =  1.720617413520813\n",
      "b =  0.6351023316383362\n",
      "80 0.17359137535095215\n",
      "w =  1.7226322889328003\n",
      "b =  0.6305220723152161\n",
      "81 0.1710965484380722\n",
      "w =  1.7246326208114624\n",
      "b =  0.6259748935699463\n",
      "82 0.16863767802715302\n",
      "w =  1.7266185283660889\n",
      "b =  0.621460497379303\n",
      "83 0.16621410846710205\n",
      "w =  1.7285901308059692\n",
      "b =  0.616978645324707\n",
      "84 0.1638251096010208\n",
      "w =  1.7305474281311035\n",
      "b =  0.6125290989875793\n",
      "85 0.16147075593471527\n",
      "w =  1.7324906587600708\n",
      "b =  0.6081116795539856\n",
      "86 0.15915018320083618\n",
      "w =  1.734419822692871\n",
      "b =  0.6037260890007019\n",
      "87 0.15686309337615967\n",
      "w =  1.7363351583480835\n",
      "b =  0.599372148513794\n",
      "88 0.15460871160030365\n",
      "w =  1.738236665725708\n",
      "b =  0.5950496196746826\n",
      "89 0.15238647162914276\n",
      "w =  1.7401244640350342\n",
      "b =  0.5907582640647888\n",
      "90 0.15019677579402924\n",
      "w =  1.7419986724853516\n",
      "b =  0.5864978432655334\n",
      "91 0.14803805947303772\n",
      "w =  1.7438592910766602\n",
      "b =  0.5822681188583374\n",
      "92 0.1459105908870697\n",
      "w =  1.745706558227539\n",
      "b =  0.5780689120292664\n",
      "93 0.14381363987922668\n",
      "w =  1.7475404739379883\n",
      "b =  0.5738999843597412\n",
      "94 0.1417466700077057\n",
      "w =  1.7493611574172974\n",
      "b =  0.5697610974311829\n",
      "95 0.13970951735973358\n",
      "w =  1.7511687278747559\n",
      "b =  0.565652072429657\n",
      "96 0.13770180940628052\n",
      "w =  1.7529633045196533\n",
      "b =  0.5615727305412292\n",
      "97 0.13572272658348083\n",
      "w =  1.7547448873519897\n",
      "b =  0.5575227737426758\n",
      "98 0.1337723284959793\n",
      "w =  1.7565135955810547\n",
      "b =  0.5535020232200623\n",
      "99 0.13184969127178192\n",
      "w =  1.7582695484161377\n",
      "b =  0.5495102405548096\n",
      "100 0.12995480000972748\n",
      "w =  1.7600128650665283\n",
      "b =  0.5455472469329834\n",
      "101 0.1280868947505951\n",
      "w =  1.7617435455322266\n",
      "b =  0.5416128635406494\n",
      "102 0.12624633312225342\n",
      "w =  1.7634618282318115\n",
      "b =  0.5377068519592285\n",
      "103 0.12443199753761292\n",
      "w =  1.7651677131652832\n",
      "b =  0.5338290333747864\n",
      "104 0.12264356762170792\n",
      "w =  1.7668612003326416\n",
      "b =  0.5299791693687439\n",
      "105 0.12088101357221603\n",
      "w =  1.7685425281524658\n",
      "b =  0.5261570811271667\n",
      "106 0.11914382129907608\n",
      "w =  1.7702118158340454\n",
      "b =  0.5223625302314758\n",
      "107 0.11743144690990448\n",
      "w =  1.7718689441680908\n",
      "b =  0.5185953378677368\n",
      "108 0.11574386805295944\n",
      "w =  1.7735141515731812\n",
      "b =  0.5148553252220154\n",
      "109 0.11408039182424545\n",
      "w =  1.775147557258606\n",
      "b =  0.5111423134803772\n",
      "110 0.11244094371795654\n",
      "w =  1.7767691612243652\n",
      "b =  0.5074560642242432\n",
      "111 0.11082485318183899\n",
      "w =  1.7783790826797485\n",
      "b =  0.503796398639679\n",
      "112 0.10923215746879578\n",
      "w =  1.7799773216247559\n",
      "b =  0.5001631379127502\n",
      "113 0.1076623797416687\n",
      "w =  1.7815641164779663\n",
      "b =  0.4965560734272003\n",
      "114 0.10611514002084732\n",
      "w =  1.7831394672393799\n",
      "b =  0.49297502636909485\n",
      "115 0.10458994656801224\n",
      "w =  1.7847033739089966\n",
      "b =  0.4894197881221771\n",
      "116 0.10308687388896942\n",
      "w =  1.7862560749053955\n",
      "b =  0.4858901798725128\n",
      "117 0.10160529613494873\n",
      "w =  1.7877975702285767\n",
      "b =  0.4823860228061676\n",
      "118 0.10014508664608002\n",
      "w =  1.7893279790878296\n",
      "b =  0.47890713810920715\n",
      "119 0.09870604425668716\n",
      "w =  1.7908473014831543\n",
      "b =  0.47545334696769714\n",
      "120 0.0972873792052269\n",
      "w =  1.7923556566238403\n",
      "b =  0.47202447056770325\n",
      "121 0.09588924050331116\n",
      "w =  1.7938531637191772\n",
      "b =  0.46862033009529114\n",
      "122 0.0945112332701683\n",
      "w =  1.795339822769165\n",
      "b =  0.4652407169342041\n",
      "123 0.09315285831689835\n",
      "w =  1.7968157529830933\n",
      "b =  0.4618854820728302\n",
      "124 0.09181419014930725\n",
      "w =  1.7982810735702515\n",
      "b =  0.4585544466972351\n",
      "125 0.09049463272094727\n",
      "w =  1.7997359037399292\n",
      "b =  0.4552474617958069\n",
      "126 0.08919411152601242\n",
      "w =  1.801180124282837\n",
      "b =  0.4519643187522888\n",
      "127 0.08791211247444153\n",
      "w =  1.8026139736175537\n",
      "b =  0.4487048387527466\n",
      "128 0.0866488665342331\n",
      "w =  1.8040374517440796\n",
      "b =  0.44546887278556824\n",
      "129 0.08540340512990952\n",
      "w =  1.805450677871704\n",
      "b =  0.44225624203681946\n",
      "130 0.0841759517788887\n",
      "w =  1.8068536520004272\n",
      "b =  0.4390667676925659\n",
      "131 0.08296631276607513\n",
      "w =  1.8082466125488281\n",
      "b =  0.43590033054351807\n",
      "132 0.08177394419908524\n",
      "w =  1.8096295595169067\n",
      "b =  0.4327567219734192\n",
      "133 0.08059869706630707\n",
      "w =  1.811002492904663\n",
      "b =  0.42963576316833496\n",
      "134 0.0794403925538063\n",
      "w =  1.8123655319213867\n",
      "b =  0.42653733491897583\n",
      "135 0.0782986730337143\n",
      "w =  1.8137186765670776\n",
      "b =  0.4234612286090851\n",
      "136 0.07717347145080566\n",
      "w =  1.8150620460510254\n",
      "b =  0.42040732502937317\n",
      "137 0.07606435567140579\n",
      "w =  1.8163957595825195\n",
      "b =  0.41737544536590576\n",
      "138 0.07497124373912811\n",
      "w =  1.8177199363708496\n",
      "b =  0.41436541080474854\n",
      "139 0.073893703520298\n",
      "w =  1.8190345764160156\n",
      "b =  0.41137710213661194\n",
      "140 0.07283182442188263\n",
      "w =  1.8203396797180176\n",
      "b =  0.40841034054756165\n",
      "141 0.07178501039743423\n",
      "w =  1.821635365486145\n",
      "b =  0.40546494722366333\n",
      "142 0.07075332850217819\n",
      "w =  1.822921633720398\n",
      "b =  0.40254080295562744\n",
      "143 0.06973662227392197\n",
      "w =  1.8241987228393555\n",
      "b =  0.39963775873184204\n",
      "144 0.0687343031167984\n",
      "w =  1.825466513633728\n",
      "b =  0.3967556357383728\n",
      "145 0.06774647533893585\n",
      "w =  1.8267252445220947\n",
      "b =  0.3938943147659302\n",
      "146 0.06677287817001343\n",
      "w =  1.827974796295166\n",
      "b =  0.39105361700057983\n",
      "147 0.06581325829029083\n",
      "w =  1.829215407371521\n",
      "b =  0.3882334232330322\n",
      "148 0.06486744433641434\n",
      "w =  1.8304470777511597\n",
      "b =  0.3854335844516754\n",
      "149 0.06393514573574066\n",
      "w =  1.8316699266433716\n",
      "b =  0.3826539218425751\n",
      "150 0.06301629543304443\n",
      "w =  1.8328838348388672\n",
      "b =  0.37989428639411926\n",
      "151 0.06211065873503685\n",
      "w =  1.8340890407562256\n",
      "b =  0.37715455889701843\n",
      "152 0.06121813878417015\n",
      "w =  1.8352855443954468\n",
      "b =  0.37443462014198303\n",
      "153 0.06033830717206001\n",
      "w =  1.8364734649658203\n",
      "b =  0.37173429131507874\n",
      "154 0.05947114899754524\n",
      "w =  1.8376528024673462\n",
      "b =  0.3690534234046936\n",
      "155 0.05861642211675644\n",
      "w =  1.8388235569000244\n",
      "b =  0.3663918673992157\n",
      "156 0.05777397379279137\n",
      "w =  1.839985966682434\n",
      "b =  0.36374953389167786\n",
      "157 0.05694371461868286\n",
      "w =  1.8411399126052856\n",
      "b =  0.36112624406814575\n",
      "158 0.05612526461482048\n",
      "w =  1.8422855138778687\n",
      "b =  0.35852187871932983\n",
      "159 0.0553186871111393\n",
      "w =  1.8434230089187622\n",
      "b =  0.35593631863594055\n",
      "160 0.05452365055680275\n",
      "w =  1.8445521593093872\n",
      "b =  0.3533693850040436\n",
      "161 0.053740110248327255\n",
      "w =  1.8456732034683228\n",
      "b =  0.35082095861434937\n",
      "162 0.052967701107263565\n",
      "w =  1.8467861413955688\n",
      "b =  0.34829092025756836\n",
      "163 0.052206505089998245\n",
      "w =  1.847891092300415\n",
      "b =  0.3457791209220886\n",
      "164 0.0514562614262104\n",
      "w =  1.8489880561828613\n",
      "b =  0.3432854413986206\n",
      "165 0.050716791301965714\n",
      "w =  1.8500771522521973\n",
      "b =  0.34080973267555237\n",
      "166 0.04998789727687836\n",
      "w =  1.8511583805084229\n",
      "b =  0.33835187554359436\n",
      "167 0.04926947131752968\n",
      "w =  1.8522318601608276\n",
      "b =  0.33591175079345703\n",
      "168 0.048561472445726395\n",
      "w =  1.853297472000122\n",
      "b =  0.33348920941352844\n",
      "169 0.047863394021987915\n",
      "w =  1.8543554544448853\n",
      "b =  0.33108416199684143\n",
      "170 0.04717554524540901\n",
      "w =  1.8554058074951172\n",
      "b =  0.32869645953178406\n",
      "171 0.046497661620378494\n",
      "w =  1.8564486503601074\n",
      "b =  0.3263259828090668\n",
      "172 0.04582935571670532\n",
      "w =  1.8574838638305664\n",
      "b =  0.32397258281707764\n",
      "173 0.04517076909542084\n",
      "w =  1.8585116863250732\n",
      "b =  0.3216361701488495\n",
      "174 0.044521622359752655\n",
      "w =  1.859532117843628\n",
      "b =  0.3193165957927704\n",
      "175 0.04388180375099182\n",
      "w =  1.8605451583862305\n",
      "b =  0.31701377034187317\n",
      "176 0.04325106367468834\n",
      "w =  1.8615508079528809\n",
      "b =  0.3147275149822235\n",
      "177 0.04262952134013176\n",
      "w =  1.8625493049621582\n",
      "b =  0.31245777010917664\n",
      "178 0.04201687127351761\n",
      "w =  1.863540530204773\n",
      "b =  0.3102043867111206\n",
      "179 0.04141297936439514\n",
      "w =  1.8645246028900146\n",
      "b =  0.30796727538108826\n",
      "180 0.040817853063344955\n",
      "w =  1.8655016422271729\n",
      "b =  0.30574628710746765\n",
      "181 0.04023119807243347\n",
      "w =  1.8664716482162476\n",
      "b =  0.30354130268096924\n",
      "182 0.039653073996305466\n",
      "w =  1.8674346208572388\n",
      "b =  0.30135223269462585\n",
      "183 0.03908313438296318\n",
      "w =  1.868390679359436\n",
      "b =  0.29917895793914795\n",
      "184 0.038521405309438705\n",
      "w =  1.8693398237228394\n",
      "b =  0.2970213294029236\n",
      "185 0.03796782344579697\n",
      "w =  1.8702821731567383\n",
      "b =  0.294879287481308\n",
      "186 0.03742213547229767\n",
      "w =  1.8712176084518433\n",
      "b =  0.2927526533603668\n",
      "187 0.03688435256481171\n",
      "w =  1.8721463680267334\n",
      "b =  0.2906413972377777\n",
      "188 0.03635431081056595\n",
      "w =  1.8730684518814087\n",
      "b =  0.2885453701019287\n",
      "189 0.035831790417432785\n",
      "w =  1.8739838600158691\n",
      "b =  0.2864644229412079\n",
      "190 0.035316888242959976\n",
      "w =  1.8748927116394043\n",
      "b =  0.28439849615097046\n",
      "191 0.03480930253863335\n",
      "w =  1.8757948875427246\n",
      "b =  0.2823474705219269\n",
      "192 0.03430899605154991\n",
      "w =  1.8766906261444092\n",
      "b =  0.2803112268447876\n",
      "193 0.03381604701280594\n",
      "w =  1.877579927444458\n",
      "b =  0.27828967571258545\n",
      "194 0.03332987055182457\n",
      "w =  1.878462791442871\n",
      "b =  0.2762826979160309\n",
      "195 0.032850973308086395\n",
      "w =  1.8793392181396484\n",
      "b =  0.27429020404815674\n",
      "196 0.03237881883978844\n",
      "w =  1.8802094459533691\n",
      "b =  0.27231210470199585\n",
      "197 0.03191351890563965\n",
      "w =  1.8810733556747437\n",
      "b =  0.2703482508659363\n",
      "198 0.031454898416996\n",
      "w =  1.8819310665130615\n",
      "b =  0.26839855313301086\n",
      "199 0.031002812087535858\n",
      "w =  1.8827825784683228\n",
      "b =  0.26646292209625244\n",
      "200 0.03055724874138832\n",
      "w =  1.8836278915405273\n",
      "b =  0.26454123854637146\n",
      "201 0.030118070542812347\n",
      "w =  1.8844671249389648\n",
      "b =  0.26263341307640076\n",
      "202 0.02968522533774376\n",
      "w =  1.8853002786636353\n",
      "b =  0.26073935627937317\n",
      "203 0.029258646070957184\n",
      "w =  1.8861274719238281\n",
      "b =  0.25885894894599915\n",
      "204 0.02883809804916382\n",
      "w =  1.8869487047195435\n",
      "b =  0.2569921016693115\n",
      "205 0.0284237302839756\n",
      "w =  1.8877639770507812\n",
      "b =  0.25513872504234314\n",
      "206 0.028015144169330597\n",
      "w =  1.888573408126831\n",
      "b =  0.25329872965812683\n",
      "207 0.027612607926130295\n",
      "w =  1.8893769979476929\n",
      "b =  0.25147199630737305\n",
      "208 0.02721576765179634\n",
      "w =  1.8901748657226562\n",
      "b =  0.24965843558311462\n",
      "209 0.026824552565813065\n",
      "w =  1.8909668922424316\n",
      "b =  0.2478579431772232\n",
      "210 0.02643909864127636\n",
      "w =  1.8917531967163086\n",
      "b =  0.24607042968273163\n",
      "211 0.026059098541736603\n",
      "w =  1.8925338983535767\n",
      "b =  0.24429582059383392\n",
      "212 0.025684580206871033\n",
      "w =  1.8933088779449463\n",
      "b =  0.24253401160240173\n",
      "213 0.02531539276242256\n",
      "w =  1.894078254699707\n",
      "b =  0.2407848984003067\n",
      "214 0.024951623752713203\n",
      "w =  1.8948421478271484\n",
      "b =  0.23904840648174286\n",
      "215 0.024593066424131393\n",
      "w =  1.8956005573272705\n",
      "b =  0.23732444643974304\n",
      "216 0.02423960343003273\n",
      "w =  1.8963534832000732\n",
      "b =  0.2356129139661789\n",
      "217 0.023891275748610497\n",
      "w =  1.8971009254455566\n",
      "b =  0.23391373455524445\n",
      "218 0.023547861725091934\n",
      "w =  1.8978430032730103\n",
      "b =  0.23222678899765015\n",
      "219 0.023209424689412117\n",
      "w =  1.898579716682434\n",
      "b =  0.23055201768875122\n",
      "220 0.022875890135765076\n",
      "w =  1.8993111848831177\n",
      "b =  0.2288893312215805\n",
      "221 0.02254711650311947\n",
      "w =  1.9000372886657715\n",
      "b =  0.22723862528800964\n",
      "222 0.022223126143217087\n",
      "w =  1.9007582664489746\n",
      "b =  0.22559982538223267\n",
      "223 0.02190372347831726\n",
      "w =  1.9014739990234375\n",
      "b =  0.2239728420972824\n",
      "224 0.02158893272280693\n",
      "w =  1.9021844863891602\n",
      "b =  0.2223576009273529\n",
      "225 0.021278642117977142\n",
      "w =  1.9028898477554321\n",
      "b =  0.2207539975643158\n",
      "226 0.02097283862531185\n",
      "w =  1.903590202331543\n",
      "b =  0.2191619724035263\n",
      "227 0.020671462640166283\n",
      "w =  1.9042855501174927\n",
      "b =  0.21758143603801727\n",
      "228 0.020374376326799393\n",
      "w =  1.9049757719039917\n",
      "b =  0.21601228415966034\n",
      "229 0.020081574097275734\n",
      "w =  1.9056611061096191\n",
      "b =  0.21445445716381073\n",
      "230 0.019792936742305756\n",
      "w =  1.9063414335250854\n",
      "b =  0.2129078507423401\n",
      "231 0.01950855553150177\n",
      "w =  1.9070168733596802\n",
      "b =  0.21137240529060364\n",
      "232 0.01922816038131714\n",
      "w =  1.9076874256134033\n",
      "b =  0.20984803140163422\n",
      "233 0.01895175687968731\n",
      "w =  1.9083532094955444\n",
      "b =  0.20833465456962585\n",
      "234 0.018679458647966385\n",
      "w =  1.9090142250061035\n",
      "b =  0.20683220028877258\n",
      "235 0.018410954624414444\n",
      "w =  1.909670352935791\n",
      "b =  0.20534054934978485\n",
      "236 0.018146414309740067\n",
      "w =  1.910321831703186\n",
      "b =  0.20385967195034027\n",
      "237 0.01788555458188057\n",
      "w =  1.910968542098999\n",
      "b =  0.20238946378231049\n",
      "238 0.017628595232963562\n",
      "w =  1.9116106033325195\n",
      "b =  0.20092988014221191\n",
      "239 0.017375214025378227\n",
      "w =  1.9122480154037476\n",
      "b =  0.1994808316230774\n",
      "240 0.01712549291551113\n",
      "w =  1.9128808975219727\n",
      "b =  0.19804221391677856\n",
      "241 0.016879340633749962\n",
      "w =  1.9135091304779053\n",
      "b =  0.19661396741867065\n",
      "242 0.016636844724416733\n",
      "w =  1.9141329526901245\n",
      "b =  0.19519604742527008\n",
      "243 0.016397681087255478\n",
      "w =  1.9147522449493408\n",
      "b =  0.1937883347272873\n",
      "244 0.016161972656846046\n",
      "w =  1.9153670072555542\n",
      "b =  0.19239075481891632\n",
      "245 0.015929752960801125\n",
      "w =  1.9159773588180542\n",
      "b =  0.19100326299667358\n",
      "246 0.015700802206993103\n",
      "w =  1.9165832996368408\n",
      "b =  0.1896257847547531\n",
      "247 0.015475135296583176\n",
      "w =  1.917184829711914\n",
      "b =  0.18825824558734894\n",
      "248 0.015252761542797089\n",
      "w =  1.9177820682525635\n",
      "b =  0.1869005709886551\n",
      "249 0.015033543109893799\n",
      "w =  1.918375015258789\n",
      "b =  0.1855526864528656\n",
      "250 0.014817497693002224\n",
      "w =  1.9189636707305908\n",
      "b =  0.1842145323753357\n",
      "251 0.014604540541768074\n",
      "w =  1.9195480346679688\n",
      "b =  0.182886004447937\n",
      "252 0.014394687488675117\n",
      "w =  1.9201282262802124\n",
      "b =  0.18156707286834717\n",
      "253 0.014187794178724289\n",
      "w =  1.9207042455673218\n",
      "b =  0.1802576631307602\n",
      "254 0.013983896002173424\n",
      "w =  1.9212760925292969\n",
      "b =  0.17895770072937012\n",
      "255 0.013782916590571404\n",
      "w =  1.9218438863754272\n",
      "b =  0.17766709625720978\n",
      "256 0.013584832660853863\n",
      "w =  1.9224075078964233\n",
      "b =  0.1763858050107956\n",
      "257 0.013389602303504944\n",
      "w =  1.9229670763015747\n",
      "b =  0.1751137673854828\n",
      "258 0.013197196647524834\n",
      "w =  1.9235225915908813\n",
      "b =  0.1738508939743042\n",
      "259 0.013007495552301407\n",
      "w =  1.9240741729736328\n",
      "b =  0.17259712517261505\n",
      "260 0.012820608913898468\n",
      "w =  1.924621820449829\n",
      "b =  0.17135240137577057\n",
      "261 0.012636302039027214\n",
      "w =  1.9251654148101807\n",
      "b =  0.1701166331768036\n",
      "262 0.012454720214009285\n",
      "w =  1.925705075263977\n",
      "b =  0.16888979077339172\n",
      "263 0.012275714427232742\n",
      "w =  1.9262408018112183\n",
      "b =  0.1676717847585678\n",
      "264 0.01209934800863266\n",
      "w =  1.9267727136611938\n",
      "b =  0.16646258533000946\n",
      "265 0.011925432831048965\n",
      "w =  1.9273008108139038\n",
      "b =  0.1652621030807495\n",
      "266 0.011754042468965054\n",
      "w =  1.9278250932693481\n",
      "b =  0.1640702784061432\n",
      "267 0.011585124768316746\n",
      "w =  1.9283456802368164\n",
      "b =  0.16288705170154572\n",
      "268 0.011418631300330162\n",
      "w =  1.928862452507019\n",
      "b =  0.16171234846115112\n",
      "269 0.01125454343855381\n",
      "w =  1.9293755292892456\n",
      "b =  0.16054612398147583\n",
      "270 0.01109277829527855\n",
      "w =  1.9298847913742065\n",
      "b =  0.15938828885555267\n",
      "271 0.010933374986052513\n",
      "w =  1.930390477180481\n",
      "b =  0.15823882818222046\n",
      "272 0.01077619194984436\n",
      "w =  1.9308924674987793\n",
      "b =  0.15709763765335083\n",
      "273 0.010621335357427597\n",
      "w =  1.9313908815383911\n",
      "b =  0.1559646725654602\n",
      "274 0.010468687862157822\n",
      "w =  1.9318856000900269\n",
      "b =  0.1548398733139038\n",
      "275 0.010318255051970482\n",
      "w =  1.9323768615722656\n",
      "b =  0.15372321009635925\n",
      "276 0.010169971734285355\n",
      "w =  1.9328645467758179\n",
      "b =  0.15261459350585938\n",
      "277 0.010023850947618484\n",
      "w =  1.9333487749099731\n",
      "b =  0.1515139639377594\n",
      "278 0.009879718534648418\n",
      "w =  1.933829426765442\n",
      "b =  0.15042127668857574\n",
      "279 0.009737765416502953\n",
      "w =  1.9343066215515137\n",
      "b =  0.14933647215366364\n",
      "280 0.009597841650247574\n",
      "w =  1.9347803592681885\n",
      "b =  0.1482594907283783\n",
      "281 0.009459836408495903\n",
      "w =  1.9352506399154663\n",
      "b =  0.14719027280807495\n",
      "282 0.009323952719569206\n",
      "w =  1.9357177019119263\n",
      "b =  0.1461287885904312\n",
      "283 0.009189932607114315\n",
      "w =  1.9361813068389893\n",
      "b =  0.14507493376731873\n",
      "284 0.009057823568582535\n",
      "w =  1.9366415739059448\n",
      "b =  0.1440286785364151\n",
      "285 0.008927715010941029\n",
      "w =  1.937098503112793\n",
      "b =  0.14298996329307556\n",
      "286 0.008799349889159203\n",
      "w =  1.9375520944595337\n",
      "b =  0.14195874333381653\n",
      "287 0.008672925643622875\n",
      "w =  1.9380024671554565\n",
      "b =  0.14093495905399323\n",
      "288 0.008548304438591003\n",
      "w =  1.9384496212005615\n",
      "b =  0.13991856575012207\n",
      "289 0.008425464853644371\n",
      "w =  1.9388935565948486\n",
      "b =  0.1389095038175583\n",
      "290 0.008304324932396412\n",
      "w =  1.9393342733383179\n",
      "b =  0.1379077136516571\n",
      "291 0.008184993639588356\n",
      "w =  1.9397717714309692\n",
      "b =  0.13691315054893494\n",
      "292 0.008067324757575989\n",
      "w =  1.9402060508728027\n",
      "b =  0.13592574000358582\n",
      "293 0.007951410487294197\n",
      "w =  1.940637230873108\n",
      "b =  0.13494546711444855\n",
      "294 0.007837142795324326\n",
      "w =  1.9410653114318848\n",
      "b =  0.13397227227687836\n",
      "295 0.007724494207650423\n",
      "w =  1.9414904117584229\n",
      "b =  0.13300611078739166\n",
      "296 0.007613468449562788\n",
      "w =  1.941912293434143\n",
      "b =  0.1320468932390213\n",
      "297 0.007504072040319443\n",
      "w =  1.9423311948776245\n",
      "b =  0.13109460473060608\n",
      "298 0.007396251428872347\n",
      "w =  1.9427471160888672\n",
      "b =  0.13014918565750122\n",
      "299 0.007289917208254337\n",
      "w =  1.943160057067871\n",
      "b =  0.12921057641506195\n",
      "300 0.007185155525803566\n",
      "w =  1.9435700178146362\n",
      "b =  0.12827874720096588\n",
      "301 0.007081935182213783\n",
      "w =  1.9439769983291626\n",
      "b =  0.12735362350940704\n",
      "302 0.006980093661695719\n",
      "w =  1.9443809986114502\n",
      "b =  0.12643517553806305\n",
      "303 0.006879799067974091\n",
      "w =  1.944782018661499\n",
      "b =  0.12552332878112793\n",
      "304 0.006780929397791624\n",
      "w =  1.9451802968978882\n",
      "b =  0.12461809068918228\n",
      "305 0.006683512590825558\n",
      "w =  1.9455755949020386\n",
      "b =  0.12371936440467834\n",
      "306 0.0065874517895281315\n",
      "w =  1.9459681510925293\n",
      "b =  0.12282713502645493\n",
      "307 0.006492751184850931\n",
      "w =  1.9463578462600708\n",
      "b =  0.12194133549928665\n",
      "308 0.0063994345255196095\n",
      "w =  1.946744680404663\n",
      "b =  0.12106190621852875\n",
      "309 0.0063074855133891106\n",
      "w =  1.9471287727355957\n",
      "b =  0.12018883973360062\n",
      "310 0.006216848734766245\n",
      "w =  1.9475101232528687\n",
      "b =  0.11932206153869629\n",
      "311 0.006127490662038326\n",
      "w =  1.9478886127471924\n",
      "b =  0.11846153438091278\n",
      "312 0.006039408501237631\n",
      "w =  1.9482643604278564\n",
      "b =  0.11760719865560532\n",
      "313 0.005952600389719009\n",
      "w =  1.9486374855041504\n",
      "b =  0.1167590469121933\n",
      "314 0.005867088679224253\n",
      "w =  1.9490078687667847\n",
      "b =  0.11591701209545135\n",
      "315 0.0057827625423669815\n",
      "w =  1.9493756294250488\n",
      "b =  0.11508104205131531\n",
      "316 0.005699664819985628\n",
      "w =  1.9497407674789429\n",
      "b =  0.11425111442804337\n",
      "317 0.005617726594209671\n",
      "w =  1.9501032829284668\n",
      "b =  0.11342716217041016\n",
      "318 0.005537000484764576\n",
      "w =  1.950463056564331\n",
      "b =  0.1126091405749321\n",
      "319 0.005457455292344093\n",
      "w =  1.9508203268051147\n",
      "b =  0.1117970421910286\n",
      "320 0.005379010923206806\n",
      "w =  1.9511749744415283\n",
      "b =  0.11099078506231308\n",
      "321 0.0053016627207398415\n",
      "w =  1.9515269994735718\n",
      "b =  0.11019032448530197\n",
      "322 0.005225492175668478\n",
      "w =  1.9518766403198242\n",
      "b =  0.10939566791057587\n",
      "323 0.005150416400283575\n",
      "w =  1.9522236585617065\n",
      "b =  0.1086067333817482\n",
      "324 0.0050763762556016445\n",
      "w =  1.9525682926177979\n",
      "b =  0.10782349854707718\n",
      "325 0.005003427620977163\n",
      "w =  1.952910304069519\n",
      "b =  0.10704588890075684\n",
      "326 0.004931529518216848\n",
      "w =  1.9532499313354492\n",
      "b =  0.10627391189336777\n",
      "327 0.004860633984208107\n",
      "w =  1.9535870552062988\n",
      "b =  0.10550747811794281\n",
      "328 0.004790782928466797\n",
      "w =  1.9539217948913574\n",
      "b =  0.10474658757448196\n",
      "329 0.0047219293192029\n",
      "w =  1.9542540311813354\n",
      "b =  0.10399116575717926\n",
      "330 0.004654097370803356\n",
      "w =  1.954584002494812\n",
      "b =  0.1032412126660347\n",
      "331 0.004587197210639715\n",
      "w =  1.954911470413208\n",
      "b =  0.10249665379524231\n",
      "332 0.004521250259131193\n",
      "w =  1.9552366733551025\n",
      "b =  0.1017574742436409\n",
      "333 0.004456277936697006\n",
      "w =  1.955559492111206\n",
      "b =  0.1010236144065857\n",
      "334 0.0043922653421759605\n",
      "w =  1.955880045890808\n",
      "b =  0.1002950593829155\n",
      "335 0.004329109564423561\n",
      "w =  1.9561982154846191\n",
      "b =  0.09957175701856613\n",
      "336 0.004266901407390833\n",
      "w =  1.9565141201019287\n",
      "b =  0.09885367006063461\n",
      "337 0.004205591976642609\n",
      "w =  1.9568277597427368\n",
      "b =  0.09814076870679855\n",
      "338 0.004145149141550064\n",
      "w =  1.9571391344070435\n",
      "b =  0.09743298590183258\n",
      "339 0.0040855491533875465\n",
      "w =  1.9574482440948486\n",
      "b =  0.0967303067445755\n",
      "340 0.00402683112770319\n",
      "w =  1.9577550888061523\n",
      "b =  0.09603270143270493\n",
      "341 0.003968973644077778\n",
      "w =  1.9580597877502441\n",
      "b =  0.09534013271331787\n",
      "342 0.003911945968866348\n",
      "w =  1.9583622217178345\n",
      "b =  0.09465254098176956\n",
      "343 0.00385569897480309\n",
      "w =  1.958662509918213\n",
      "b =  0.0939699187874794\n",
      "344 0.003800296690315008\n",
      "w =  1.9589606523513794\n",
      "b =  0.0932922288775444\n",
      "345 0.0037456615827977657\n",
      "w =  1.959256649017334\n",
      "b =  0.09261941909790039\n",
      "346 0.003691853489726782\n",
      "w =  1.9595504999160767\n",
      "b =  0.09195145964622498\n",
      "347 0.003638799302279949\n",
      "w =  1.9598422050476074\n",
      "b =  0.09128830581903458\n",
      "348 0.0035865087993443012\n",
      "w =  1.9601317644119263\n",
      "b =  0.0906299501657486\n",
      "349 0.0035349386744201183\n",
      "w =  1.9604192972183228\n",
      "b =  0.08997634053230286\n",
      "350 0.0034841608721762896\n",
      "w =  1.9607046842575073\n",
      "b =  0.08932743966579437\n",
      "351 0.003434097860008478\n",
      "w =  1.9609880447387695\n",
      "b =  0.08868323266506195\n",
      "352 0.0033847223967313766\n",
      "w =  1.9612693786621094\n",
      "b =  0.0880436822772026\n",
      "353 0.0033360866364091635\n",
      "w =  1.9615486860275269\n",
      "b =  0.08740873634815216\n",
      "354 0.0032881530933082104\n",
      "w =  1.961825966835022\n",
      "b =  0.08677837997674942\n",
      "355 0.003240884281694889\n",
      "w =  1.9621013402938843\n",
      "b =  0.0861525610089302\n",
      "356 0.0031943325884640217\n",
      "w =  1.9623746871948242\n",
      "b =  0.08553124219179153\n",
      "357 0.003148401156067848\n",
      "w =  1.9626460075378418\n",
      "b =  0.08491440862417221\n",
      "358 0.003103165188804269\n",
      "w =  1.9629154205322266\n",
      "b =  0.08430203050374985\n",
      "359 0.0030585522763431072\n",
      "w =  1.9631829261779785\n",
      "b =  0.08369406312704086\n",
      "360 0.0030145947821438313\n",
      "w =  1.963448405265808\n",
      "b =  0.08309047669172287\n",
      "361 0.002971287816762924\n",
      "w =  1.9637119770050049\n",
      "b =  0.08249124139547348\n",
      "362 0.0029285491909831762\n",
      "w =  1.9639736413955688\n",
      "b =  0.08189631998538971\n",
      "363 0.002886494854465127\n",
      "w =  1.9642333984375\n",
      "b =  0.08130569756031036\n",
      "364 0.002845014678314328\n",
      "w =  1.964491367340088\n",
      "b =  0.08071935176849365\n",
      "365 0.0028041242621839046\n",
      "w =  1.964747428894043\n",
      "b =  0.0801372230052948\n",
      "366 0.002763820346444845\n",
      "w =  1.9650017023086548\n",
      "b =  0.07955929636955261\n",
      "367 0.002724078018218279\n",
      "w =  1.9652540683746338\n",
      "b =  0.0789855346083641\n",
      "368 0.002684924751520157\n",
      "w =  1.9655046463012695\n",
      "b =  0.07841590791940689\n",
      "369 0.002646359847858548\n",
      "w =  1.965753436088562\n",
      "b =  0.07785040140151978\n",
      "370 0.00260834745131433\n",
      "w =  1.9660004377365112\n",
      "b =  0.07728896290063858\n",
      "371 0.002570842858403921\n",
      "w =  1.9662456512451172\n",
      "b =  0.07673157006502151\n",
      "372 0.0025339024141430855\n",
      "w =  1.9664890766143799\n",
      "b =  0.07617819309234619\n",
      "373 0.0024974821135401726\n",
      "w =  1.9667307138442993\n",
      "b =  0.07562880963087082\n",
      "374 0.0024615938309580088\n",
      "w =  1.966970682144165\n",
      "b =  0.07508338987827301\n",
      "375 0.0024262117221951485\n",
      "w =  1.9672088623046875\n",
      "b =  0.07454190403223038\n",
      "376 0.0023913299664855003\n",
      "w =  1.9674453735351562\n",
      "b =  0.07400432229042053\n",
      "377 0.0023569860495626926\n",
      "w =  1.9676802158355713\n",
      "b =  0.07347062230110168\n",
      "378 0.00232308660633862\n",
      "w =  1.967913269996643\n",
      "b =  0.07294075936079025\n",
      "379 0.0022896970622241497\n",
      "w =  1.9681446552276611\n",
      "b =  0.07241471856832504\n",
      "380 0.0022567841224372387\n",
      "w =  1.9683743715286255\n",
      "b =  0.07189247012138367\n",
      "381 0.0022243699058890343\n",
      "w =  1.9686024188995361\n",
      "b =  0.07137399911880493\n",
      "382 0.00219239154830575\n",
      "w =  1.968828797340393\n",
      "b =  0.07085926830768585\n",
      "383 0.0021608881652355194\n",
      "w =  1.9690536260604858\n",
      "b =  0.07034824788570404\n",
      "384 0.002129835309460759\n",
      "w =  1.969276785850525\n",
      "b =  0.0698409229516983\n",
      "385 0.0020992523059248924\n",
      "w =  1.9694983959197998\n",
      "b =  0.06933724880218506\n",
      "386 0.002069062553346157\n",
      "w =  1.969718337059021\n",
      "b =  0.06883721053600311\n",
      "387 0.0020393459126353264\n",
      "w =  1.969936728477478\n",
      "b =  0.06834077835083008\n",
      "388 0.002010032068938017\n",
      "w =  1.970153570175171\n",
      "b =  0.06784792989492416\n",
      "389 0.001981152920052409\n",
      "w =  1.9703688621520996\n",
      "b =  0.06735863536596298\n",
      "390 0.0019526728428900242\n",
      "w =  1.9705826044082642\n",
      "b =  0.06687285751104355\n",
      "391 0.001924590440467\n",
      "w =  1.970794677734375\n",
      "b =  0.06639056652784348\n",
      "392 0.0018969366792589426\n",
      "w =  1.9710053205490112\n",
      "b =  0.06591177731752396\n",
      "393 0.0018696942133828998\n",
      "w =  1.9712144136428833\n",
      "b =  0.06543643027544022\n",
      "394 0.0018428112380206585\n",
      "w =  1.9714219570159912\n",
      "b =  0.06496451050043106\n",
      "395 0.00181634072214365\n",
      "w =  1.9716280698776245\n",
      "b =  0.06449601799249649\n",
      "396 0.001790214329957962\n",
      "w =  1.9718327522277832\n",
      "b =  0.06403089314699173\n",
      "397 0.001764484797604382\n",
      "w =  1.9720358848571777\n",
      "b =  0.0635690987110138\n",
      "398 0.001739130588248372\n",
      "w =  1.9722375869750977\n",
      "b =  0.06311064958572388\n",
      "399 0.0017141285352408886\n",
      "w =  1.9724377393722534\n",
      "b =  0.0626554936170578\n",
      "400 0.0016895177541300654\n",
      "w =  1.9726364612579346\n",
      "b =  0.06220363453030586\n",
      "401 0.0016652202466502786\n",
      "w =  1.9728338718414307\n",
      "b =  0.061755046248435974\n",
      "402 0.0016412705881521106\n",
      "w =  1.9730297327041626\n",
      "b =  0.06130966916680336\n",
      "403 0.0016176931094378233\n",
      "w =  1.9732242822647095\n",
      "b =  0.060867518186569214\n",
      "404 0.0015944511396810412\n",
      "w =  1.9734174013137817\n",
      "b =  0.06042855605483055\n",
      "405 0.0015715248882770538\n",
      "w =  1.9736090898513794\n",
      "b =  0.05999275669455528\n",
      "406 0.001548938569612801\n",
      "w =  1.973799467086792\n",
      "b =  0.059560101479291916\n",
      "407 0.001526692882180214\n",
      "w =  1.97398841381073\n",
      "b =  0.059130556881427765\n",
      "408 0.0015047478955239058\n",
      "w =  1.974176049232483\n",
      "b =  0.05870411545038223\n",
      "409 0.001483120839111507\n",
      "w =  1.9743622541427612\n",
      "b =  0.05828074738383293\n",
      "410 0.0014618104323744774\n",
      "w =  1.9745471477508545\n",
      "b =  0.05786043033003807\n",
      "411 0.0014408064307644963\n",
      "w =  1.9747307300567627\n",
      "b =  0.057443153113126755\n",
      "412 0.001420083804987371\n",
      "w =  1.9749128818511963\n",
      "b =  0.0570288710296154\n",
      "413 0.0013996721245348454\n",
      "w =  1.9750938415527344\n",
      "b =  0.05661759525537491\n",
      "414 0.0013795595150440931\n",
      "w =  1.9752734899520874\n",
      "b =  0.056209273636341095\n",
      "415 0.00135974888689816\n",
      "w =  1.9754518270492554\n",
      "b =  0.05580390617251396\n",
      "416 0.00134020927362144\n",
      "w =  1.9756288528442383\n",
      "b =  0.055401455610990524\n",
      "417 0.0013209484750404954\n",
      "w =  1.9758045673370361\n",
      "b =  0.05500191077589989\n",
      "418 0.0013019550824537873\n",
      "w =  1.9759790897369385\n",
      "b =  0.05460524931550026\n",
      "419 0.0012832303764298558\n",
      "w =  1.9761523008346558\n",
      "b =  0.05421143025159836\n",
      "420 0.0012648026458919048\n",
      "w =  1.9763243198394775\n",
      "b =  0.05382046848535538\n",
      "421 0.0012466066982597113\n",
      "w =  1.9764950275421143\n",
      "b =  0.05343231186270714\n",
      "422 0.0012286913115531206\n",
      "w =  1.9766645431518555\n",
      "b =  0.05304696038365364\n",
      "423 0.0012110432144254446\n",
      "w =  1.9768328666687012\n",
      "b =  0.052664387971162796\n",
      "424 0.0011936393566429615\n",
      "w =  1.9769999980926514\n",
      "b =  0.05228458717465401\n",
      "425 0.0011764870723709464\n",
      "w =  1.9771658182144165\n",
      "b =  0.05190751329064369\n",
      "426 0.0011595678515732288\n",
      "w =  1.9773304462432861\n",
      "b =  0.05153316631913185\n",
      "427 0.0011429106816649437\n",
      "w =  1.9774938821792603\n",
      "b =  0.0511615164577961\n",
      "428 0.0011264891363680363\n",
      "w =  1.9776562452316284\n",
      "b =  0.05079256370663643\n",
      "429 0.0011103007709607482\n",
      "w =  1.977817416191101\n",
      "b =  0.050426263362169266\n",
      "430 0.0010943468660116196\n",
      "w =  1.9779773950576782\n",
      "b =  0.05006259307265282\n",
      "431 0.0010786058846861124\n",
      "w =  1.9781361818313599\n",
      "b =  0.04970155283808708\n",
      "432 0.0010631077457219362\n",
      "w =  1.9782938957214355\n",
      "b =  0.04934311285614967\n",
      "433 0.0010478366166353226\n",
      "w =  1.9784504175186157\n",
      "b =  0.0489872582256794\n",
      "434 0.0010327694471925497\n",
      "w =  1.97860586643219\n",
      "b =  0.04863397032022476\n",
      "435 0.0010179245145991445\n",
      "w =  1.9787601232528687\n",
      "b =  0.048283226788043976\n",
      "436 0.0010033141588792205\n",
      "w =  1.9789133071899414\n",
      "b =  0.04793502017855644\n",
      "437 0.0009888861095532775\n",
      "w =  1.9790654182434082\n",
      "b =  0.04758932441473007\n",
      "438 0.0009746733703650534\n",
      "w =  1.9792163372039795\n",
      "b =  0.047246117144823074\n",
      "439 0.0009606824023649096\n",
      "w =  1.9793663024902344\n",
      "b =  0.04690539836883545\n",
      "440 0.000946864194702357\n",
      "w =  1.9795150756835938\n",
      "b =  0.04656711220741272\n",
      "441 0.0009332598419860005\n",
      "w =  1.9796627759933472\n",
      "b =  0.04623128101229668\n",
      "442 0.0009198525222018361\n",
      "w =  1.9798094034194946\n",
      "b =  0.045897867530584335\n",
      "443 0.0009066122584044933\n",
      "w =  1.9799550771713257\n",
      "b =  0.0455668680369854\n",
      "444 0.0008935914374887943\n",
      "w =  1.9800996780395508\n",
      "b =  0.04523824527859688\n",
      "445 0.0008807528065517545\n",
      "w =  1.98024320602417\n",
      "b =  0.04491199925541878\n",
      "446 0.0008680978789925575\n",
      "w =  1.980385661125183\n",
      "b =  0.04458809643983841\n",
      "447 0.0008556263637728989\n",
      "w =  1.9805270433425903\n",
      "b =  0.04426652565598488\n",
      "448 0.0008433125913143158\n",
      "w =  1.9806674718856812\n",
      "b =  0.043947283178567886\n",
      "449 0.0008312087738886476\n",
      "w =  1.9808069467544556\n",
      "b =  0.04363035038113594\n",
      "450 0.0008192507084459066\n",
      "w =  1.980945348739624\n",
      "b =  0.04331570118665695\n",
      "451 0.000807480129878968\n",
      "w =  1.981082797050476\n",
      "b =  0.04300331696867943\n",
      "452 0.0007958782371133566\n",
      "w =  1.9812192916870117\n",
      "b =  0.04269319027662277\n",
      "453 0.0007844336796551943\n",
      "w =  1.9813547134399414\n",
      "b =  0.0423852875828743\n",
      "454 0.0007731674122624099\n",
      "w =  1.9814891815185547\n",
      "b =  0.042079612612724304\n",
      "455 0.0007620563847012818\n",
      "w =  1.9816226959228516\n",
      "b =  0.0417761392891407\n",
      "456 0.0007510991999879479\n",
      "w =  1.981755256652832\n",
      "b =  0.0414748452603817\n",
      "457 0.0007403019699268043\n",
      "w =  1.981886863708496\n",
      "b =  0.041175726801157\n",
      "458 0.0007296661497093737\n",
      "w =  1.9820173978805542\n",
      "b =  0.04087875410914421\n",
      "459 0.0007191727636381984\n",
      "w =  1.9821470975875854\n",
      "b =  0.04058394953608513\n",
      "460 0.0007088405545800924\n",
      "w =  1.9822758436203003\n",
      "b =  0.04029126092791557\n",
      "461 0.0006986506632529199\n",
      "w =  1.9824036359786987\n",
      "b =  0.04000069200992584\n",
      "462 0.0006886154878884554\n",
      "w =  1.9825304746627808\n",
      "b =  0.03971220925450325\n",
      "463 0.0006787195452488959\n",
      "w =  1.982656478881836\n",
      "b =  0.039425816386938095\n",
      "464 0.0006689643487334251\n",
      "w =  1.9827815294265747\n",
      "b =  0.03914148733019829\n",
      "465 0.00065934396116063\n",
      "w =  1.9829057455062866\n",
      "b =  0.03885921090841293\n",
      "466 0.0006498734001070261\n",
      "w =  1.9830290079116821\n",
      "b =  0.03857896849513054\n",
      "467 0.0006405242020264268\n",
      "w =  1.9831514358520508\n",
      "b =  0.03830075263977051\n",
      "468 0.0006313319317996502\n",
      "w =  1.983272910118103\n",
      "b =  0.03802454471588135\n",
      "469 0.0006222626543603837\n",
      "w =  1.9833935499191284\n",
      "b =  0.037750326097011566\n",
      "470 0.0006133209099061787\n",
      "w =  1.983513355255127\n",
      "b =  0.03747808188199997\n",
      "471 0.000604495347943157\n",
      "w =  1.983632206916809\n",
      "b =  0.03720780089497566\n",
      "472 0.0005958104156889021\n",
      "w =  1.9837502241134644\n",
      "b =  0.03693947196006775\n",
      "473 0.000587248825468123\n",
      "w =  1.9838674068450928\n",
      "b =  0.03667307645082474\n",
      "474 0.0005788150010630488\n",
      "w =  1.9839837551116943\n",
      "b =  0.03640860691666603\n",
      "475 0.0005705018993467093\n",
      "w =  1.984099268913269\n",
      "b =  0.036146052181720734\n",
      "476 0.0005622916505672038\n",
      "w =  1.984213948249817\n",
      "b =  0.03588537871837616\n",
      "477 0.0005542148137465119\n",
      "w =  1.984327793121338\n",
      "b =  0.035626575350761414\n",
      "478 0.0005462521803565323\n",
      "w =  1.984440803527832\n",
      "b =  0.03536964952945709\n",
      "479 0.000538392981979996\n",
      "w =  1.9845529794692993\n",
      "b =  0.03511456400156021\n",
      "480 0.0005306550301611423\n",
      "w =  1.9846643209457397\n",
      "b =  0.03486132621765137\n",
      "481 0.0005230262759141624\n",
      "w =  1.9847749471664429\n",
      "b =  0.03460992872714996\n",
      "482 0.0005155227263458073\n",
      "w =  1.9848847389221191\n",
      "b =  0.03436034172773361\n",
      "483 0.0005081100971437991\n",
      "w =  1.984993815422058\n",
      "b =  0.03411255031824112\n",
      "484 0.0005008123116567731\n",
      "w =  1.9851020574569702\n",
      "b =  0.03386653959751129\n",
      "485 0.0004936138284392655\n",
      "w =  1.9852094650268555\n",
      "b =  0.03362230211496353\n",
      "486 0.0004865136288572103\n",
      "w =  1.9853161573410034\n",
      "b =  0.03337983042001724\n",
      "487 0.00047952341265045106\n",
      "w =  1.9854220151901245\n",
      "b =  0.033139098435640335\n",
      "488 0.0004726305487565696\n",
      "w =  1.9855271577835083\n",
      "b =  0.032900113612413406\n",
      "489 0.00046583529911004007\n",
      "w =  1.9856314659118652\n",
      "b =  0.03266283869743347\n",
      "490 0.0004591452598106116\n",
      "w =  1.9857350587844849\n",
      "b =  0.03242729231715202\n",
      "491 0.0004525383119471371\n",
      "w =  1.9858379364013672\n",
      "b =  0.03219345211982727\n",
      "492 0.0004460393392946571\n",
      "w =  1.9859400987625122\n",
      "b =  0.03196128457784653\n",
      "493 0.0004396276781335473\n",
      "w =  1.98604154586792\n",
      "b =  0.03173078969120979\n",
      "494 0.000433319277362898\n",
      "w =  1.9861422777175903\n",
      "b =  0.03150196000933647\n",
      "495 0.000427085324190557\n",
      "w =  1.9862421751022339\n",
      "b =  0.03127476945519447\n",
      "496 0.0004209454345982522\n",
      "w =  1.9863413572311401\n",
      "b =  0.031049231067299843\n",
      "497 0.00041489623254165053\n",
      "w =  1.9864399433135986\n",
      "b =  0.03082532249391079\n",
      "498 0.000408935418818146\n",
      "w =  1.9865376949310303\n",
      "b =  0.030603012070059776\n",
      "499 0.0004030621494166553\n",
      "w =  1.9866348505020142\n",
      "b =  0.030382318422198296\n",
      "y_pred =  tensor([[7.9769]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epoch')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# prepare dataset\n",
    "# x,y是矩阵，3行1列 也就是说总共有3个数据，每个数据只有1个特征\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])\n",
    " \n",
    "#design model using class\n",
    "\"\"\"\n",
    "our model class should be inherit from nn.Module, which is base class for all neural network modules.\n",
    "member methods __init__() and forward() have to be implemented\n",
    "class nn.linear contain two member Tensors: weight and bias\n",
    "class nn.Linear has implemented the magic method __call__(),which enable the instance of the class can\n",
    "be called just like a function.Normally the forward() will be called \n",
    "\"\"\"\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        # (1,1)是指输入x和输出y的特征维度，这里数据集中的x和y的特征都是1维的\n",
    "        # 该线性层需要学习的参数是w和b  获取w/b的方式分别是~linear.weight/linear.bias\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = LinearModel()\n",
    " \n",
    "# construct loss and optimizer\n",
    "# criterion = torch.nn.MSELoss(size_average = False) # 不需要对loss做均值处理\n",
    "criterion = torch.nn.MSELoss(reduction = 'sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) # model.parameters() 即使是嵌套，也能找到所有的参数，自动完成参数的初始化操作，这个地方我可能理解错了\n",
    " \n",
    "loss_list = []\n",
    "# training cycle forward, backward, update\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x_data) # forward:predict\n",
    "    loss = criterion(y_pred, y_data) # forward: loss\n",
    "    print(epoch, loss.item()) # loss是一个对象，但是print的时候会自动调用__str__()函数\n",
    " \n",
    "    optimizer.zero_grad() # the grad computer by .backward() will be accumulated. so before backward, remember set the grad to zero\n",
    "    loss.backward() # backward: autograd，自动计算梯度\n",
    "    optimizer.step() # update 参数，即更新w和b的值\n",
    "    print('w = ', model.linear.weight.item())\n",
    "    print('b = ', model.linear.bias.item())\n",
    "    \n",
    "    loss_list.append(loss.item())\n",
    "\n",
    "x_test = torch.tensor([[4.0]])\n",
    "y_test = model(x_test)\n",
    "print('y_pred = ', y_test.data)\n",
    "\n",
    "plt.plot(range(500),loss_list)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
